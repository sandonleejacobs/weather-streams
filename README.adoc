= Weather Streams

Using NOAA APi data to showcase features of Confluent Cloud - including Apache Kafka, Kafka Connect,
Confluent Schema Registry, Flink SQL on Confluent Cloud, and Terraform Confluent Provider.

== Sourcing Data

=== NOAA Weather APIs

The National Oceanic and Atmospheric Administration provides a free API with access to forecasts, alerts and observations.
https://www.weather.gov/documentation/services-web-api[Per their documentation]:

[quote]
All of the information presented via the API is intended to be open data, free to use for any purpose. As a public service of the United States Government, we do not charge any fees for the usage of this service, although there are reasonable rate limits in place to prevent abuse and help ensure that everyone has access. The rate limit is not public information, but allows a generous amount for typical use. If the rate limit is execeed a request will return with an error, and may be retried after the limit clears (typically within 5 seconds). Proxies are more likely to reach the limit, whereas requests directly from clients are not likely.

.For purposes of this demo, we plan to use:
* `/zones` - get the latest Zone definitions. These are fairly "static" in nature, so we will infrequently request this data.
* `/alerts/active` - get ACTIVE weather alerts. This will include updates and cancellations of those alerts.

=== Confluent Cloud Assets

Given this data is exposed via an HTTP endpoint, we can utilize the HTTP Source Connector to load the results of these
endpoints into Kafka topics. I chose to use the Confluent Terraform Provider to provision the Confluent Cloud resources
needed for this demo.

==== Confluent Cloud Environment
Let's start by provisioning a Confluent Cloud Environment, including a Kafka cluster and Schema Registry.

In terraform, our environment looks like this:
```
resource "confluent_environment" "cc_env" {
  display_name = var.cc_env_display_name

  lifecycle {
    prevent_destroy = false
  }
}
```

Now we can provision a Kafka cluster and Schema Registry:
[%collapsible]
====
```
resource "confluent_kafka_cluster" "basic" {
  display_name = var.cc_cluster_name
  availability = "SINGLE_ZONE"
  cloud        = var.cloud_provider
  region       = var.cloud_region
  basic {}
  environment {
    id = confluent_environment.cc_env.id
  }
}
...

resource "confluent_schema_registry_cluster" "essentials" {
  package = data.confluent_schema_registry_region.essentials.package

  environment {
    id = confluent_environment.cc_env.id
  }

  region {
    id = data.confluent_schema_registry_region.essentials.id
  }
}
```
====


==== Kafka Topics and Schemas
We should create the Kafka topics to which we want to land the data from our API calls.

For Zones, we create a compacted Kafka topic named `NoaaZonesInbound` with 3 partitions:
[%collapsible]
====
```
resource "confluent_kafka_topic" "noaa_zones_inbound" {
  kafka_cluster {
    id = confluent_kafka_cluster.basic.id
  }

  topic_name    = "NoaaZonesInbound"
  rest_endpoint = confluent_kafka_cluster.basic.rest_endpoint
  credentials {
    key    = confluent_api_key.app-manager-kafka-api-key.id
    secret = confluent_api_key.app-manager-kafka-api-key.secret
  }

  partitions_count = 3
  config = {
    "cleanup.policy" = "compact"
  }

  depends_on = [
    confluent_schema_registry_cluster.essentials
  ]
}
```
====

For our Active Alerts data, we create a compacted Kafka topic named `NoaaAlertsActiveInbound` with 6 partitons:
[%collapsible]
====
```
resource "confluent_kafka_topic" "noaa_alerts_active_inbound" {
  kafka_cluster {
    id = confluent_kafka_cluster.basic.id
  }

  topic_name       = "NoaaAlertsActiveInbound"
  rest_endpoint      = confluent_kafka_cluster.basic.rest_endpoint
  credentials {
    key    = confluent_api_key.app-manager-kafka-api-key.id
    secret = confluent_api_key.app-manager-kafka-api-key.secret
  }

  partitions_count = 3
  config = {
    "cleanup.policy" = "compact"
  }
  lifecycle {
    prevent_destroy = false
  }

  depends_on = [
    confluent_schema_registry_cluster.essentials
  ]
}
```
====

==== HTTP Source Connectors
How we're ready to source the data from the NOAA APIs into Kafka. To do so, let's create a couple of HTTP Source Connectors
Yes, with terraform...

Every 30 minutes, let's use the `/zones` endpoint to update the Zone definitions in Kafka. This connector polls the
endpoint, iterates over the `features` collection returned from the call, and writes those events to the `NoaaZonesInbound`
topic defined above:
[%collapsible]
====
```
resource "confluent_connector" "noaa_zones_source" {
  environment {
    id = confluent_environment.cc_env.id
  }
  kafka_cluster {
    id = confluent_kafka_cluster.basic.id
  }

  config_sensitive = {}

  config_nonsensitive = {
    "connector.class"          = "HttpSource"
    "name"                     = "http_src_noaa_zones"
    "kafka.auth.mode"          = "KAFKA_API_KEY"
    "kafka.api.key"            = confluent_api_key.app-manager-kafka-api-key.id
    "kafka.api.secret"         = confluent_api_key.app-manager-kafka-api-key.secret
    "kafka.topic"              = confluent_kafka_topic.noaa_zones_inbound.topic_name
    "output.data.format"       = "AVRO"
    "tasks.max"                = "1"
    "topic.name.pattern"       = confluent_kafka_topic.noaa_zones_inbound.topic_name
    "url"                      = "https://api.weather.gov/zones"
    "http.request.parameters"  = "type=land"
    "http.offset.mode"         = "SIMPLE_INCREMENTING"
    "http.initial.offset"      = "0"
    "http.response.data.json.pointer" = "/features"
    "request.interval.ms"      = 1800000

...
}
```
====

We also want to pull in active alerts, every 60 seconds since this data changes over time. This connector polls the
active alerts endpoint, iterating over the `/features` collection, and writes those records to the `NoaaAlertsActiveInbound`
kafka topic:
[%collapsible]
====
```
resource "confluent_connector" "noaa_alerts_source" {
  environment {
    id = confluent_environment.cc_env.id
  }
  kafka_cluster {
    id = confluent_kafka_cluster.basic.id
  }

  config_sensitive = {}

  config_nonsensitive = {
    "connector.class"          = "HttpSource"
    "name"                     = "http_src_noaa_alerts"
    "kafka.auth.mode"          = "KAFKA_API_KEY"
    "kafka.api.key"            = confluent_api_key.app-manager-kafka-api-key.id
    "kafka.api.secret"         = confluent_api_key.app-manager-kafka-api-key.secret
    "kafka.topic"              = confluent_kafka_topic.noaa_alerts_active_inbound.topic_name
    "output.data.format"       = "AVRO"
    "tasks.max"                = "1"
    "topic.name.pattern"       = confluent_kafka_topic.noaa_alerts_active_inbound.topic_name
    "url"                      = "https://api.weather.gov/alerts/active"
    "http.request.parameters"  = "status=actual&limit=10"
    "http.offset.mode"         = "SIMPLE_INCREMENTING"
    "http.initial.offset"      = "0"
    "http.response.data.json.pointer" = "/features"
    "request.interval.ms"      = 60000
...
}
```
====

==== Active Data Transformation
So iterating over the `/features` collections in these scenarios is not enough to "shape" the data how we need it for
future purposes. To do that, let's explore the world of https://docs.confluent.io/cloud/current/connectors/transforms/overview.html[Single Message Transformations (SMT) in Kafka Connect].

For the Zones data, we use a series of SMTs to:
[arabic]
* There are fields we want to drop from the start - mainly because the naming convention and because those values are
reflected elsewhere in the dataset.
* Some of the data we need for our purposes is "nested" in other Structs, so let's flatten those elements.
* Once flattened, we can rename those fields to make more sense in the ensuing Kafka data.
* Finally, we need to create a key for the Kafka events written to the compacted topic.

[%collapsible]
====
```
"transforms"                            = "DropUnusedFields,Flatten,RenameFields,MakeEventKey"
"transforms.DropUnusedFields.type"      = "org.apache.kafka.connect.transforms.ReplaceField$Value"
"transforms.DropUnusedFields.exclude"   = "id,type"

"transforms.Flatten.type"               = "org.apache.kafka.connect.transforms.Flatten$Value"
"transforms.Flatten.delimiter"          = "_"

"transforms.RenameFields.type"          = "org.apache.kafka.connect.transforms.ReplaceField$Value"
"transforms.RenameFields.renames"       = "properties_@id:url,properties_@type:wxObjectType,properties_id:id,properties_type:zoneType,properties_name:name,properties_effectiveDate:effectiveDate,properties_expirationDate:expirationDate,properties_state:state,properties_cwa:cwas,properties_forecastOffices:forecastOffices,properties_timeZone:timeZones,properties_observationStations:observationStations,properties_radarStation:radarStation"

"transforms.MakeEventKey.type"          = "org.apache.kafka.connect.transforms.ValueToKey"
"transforms.MakeEventKey.fields"        = "id"
```
====

For the active alerts data, we only need one transformation - to create a key for the Kafka events written to the compacted topic.
[%collapsible]
====
```
"transforms"                            = "MakeEventKey"
"transforms.MakeEventKey.type"          = "org.apache.kafka.connect.transforms.ValueToKey"
"transforms.MakeEventKey.fields"        = "id"
```
====
